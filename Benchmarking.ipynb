{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033b70b6-07e9-494f-890d-47231a2f620c",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60013e7d-b8de-4e14-829a-2833d0bf0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmakring on 30% of data\n",
    "# loop crashes in between configs due to no memory left so manual \"cleaning\" from terminal was done in between thsoe\n",
    "# so csv was overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667a1f2d-fd38-47f9-a738-4657c172ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.srop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2bc744-6e8f-4ffa-93d0-5d244cebe499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when, collect_list\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "import struct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from statistics import mean\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from matplotlib.patches import Rectangle\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2e911f-fbd6-4f6b-8202-80167169f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867682e5-0669-4a87-920d-37b268ad4e2b",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36f70f3-36de-4c69-86e3-3a46f7b3a241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(file_content):\n",
    "    word_size = 8\n",
    "    words = (file_content[i:i+word_size] for i in range(0, len(file_content), word_size))\n",
    "    unpacked = []\n",
    "    for word in words:\n",
    "        word = struct.unpack('<q', word)[0]\n",
    "        if (word >> 61) & 0x7 == 2:\n",
    "            unpacked.append([\n",
    "                (word >> 58) & 0x7,\n",
    "                (word >> 49) & 0x1FF,\n",
    "                (word >> 17) & 0xFFFFFFFF,\n",
    "                (word >> 5) & 0xFFF,\n",
    "                (word >> 0) & 0x1F\n",
    "            ])\n",
    "    return unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d24cb86f-ea17-45c5-b3dc-6f368c6b1278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_fit_rdd(points):\n",
    "\n",
    "    points = list(points)\n",
    "    if not points or len(points) < 2:\n",
    "        return None\n",
    "\n",
    "    x_lefts = [p[0] for p in points]\n",
    "    x_rights = [p[1] for p in points]\n",
    "    z_vals = np.array([p[2] for p in points])\n",
    "\n",
    "    try:\n",
    "        combinations = list(product(*zip(x_lefts, x_rights)))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    best_m, best_b = None, None\n",
    "    min_residuals = float(\"inf\")\n",
    "\n",
    "    for combo in combinations:\n",
    "        x_vals = np.array(combo)\n",
    "        A = np.vstack([x_vals, np.ones(len(x_vals))]).T\n",
    "        m, b = np.linalg.lstsq(A, z_vals, rcond=None)[0]\n",
    "        residuals = np.linalg.norm(z_vals - (m * x_vals + b))\n",
    "\n",
    "        if residuals < min_residuals:\n",
    "            min_residuals = residuals\n",
    "            best_m, best_b = m, b\n",
    "            combin = combo\n",
    "\n",
    "    return (best_m, best_b, min_residuals, combin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f6e1f6-fb8d-42e5-87b7-aa3895bac763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_b(xs: list, threshold: float = 4*42, k: int = 2):\n",
    "    xs = np.array(xs, dtype=float)\n",
    "    differences = np.abs(xs[:, None] - xs)\n",
    "    count_exceeding = np.sum(differences > threshold, axis=1)\n",
    "    return count_exceeding < k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb07906e-eafb-4e16-8921-8cbb0ad06b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_fit(points):\n",
    "    X = [p[0] for p in points]\n",
    "    Z = [p[1] for p in points]\n",
    "    m, b = np.polyfit(X, Z, 1)\n",
    "    return m, b, X, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90533a3c-25c6-4050-a2ef-94c1e4cdf759",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "667e9296-a499-49d0-97a5-2f02e3d26ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# executors_list = [2,3,4,6]\n",
    "# max_cores_list = [2,4,6,12]\n",
    "# partitions_list = [2,4,8,12]\n",
    "\n",
    "# executors_list = [1]\n",
    "# max_cores_list = [2,6,12]\n",
    "# partitions_list = [2,4,8,12]\n",
    "\n",
    "executors_list = [6]\n",
    "max_cores_list = [12]\n",
    "partitions_list = [2,4,8,12]\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5b849-fe5e-45b6-9b72-a06c7e74f6d3",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e2a519-e203-451c-b267-817860e8d43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: com.amazonaws.sdk.disableCertChecking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/qasim/.ivy2/cache\n",
      "The jars for the packages stored in: /home/qasim/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a5f41757-1d48-49bc-927c-b794ae9b2aa5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.43.v20210629 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound ch.qos.reload4j#reload4j;1.2.22 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound org.slf4j#slf4j-reload4j;1.7.36 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.apache.commons#commons-compress;1.21 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.4 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.4.7 in central\n",
      "\tfound net.minidev#accessors-smart;2.4.7 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.55 in central\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.7 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 1031ms :: artifacts dl 34ms\n",
      "\t:: modules in use:\n",
      "\tch.qos.reload4j#reload4j;1.2.22 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.7 from central in [default]\n",
      "\tnet.minidev#json-smart;2.4.7 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.43.v20210629 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.slf4j#slf4j-reload4j;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   95  |   0   |   0   |   0   ||   95  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a5f41757-1d48-49bc-927c-b794ae9b2aa5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 95 already retrieved (0kB/20ms)\n",
      "25/10/14 21:46:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/14 21:46:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/10/14 21:46:33 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/10/14 21:46:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/10/14 21:47:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "for executors in executors_list:\n",
    "    for max_cores in max_cores_list:\n",
    "        for partitions in partitions_list:\n",
    "\n",
    "            unpack_times = []\n",
    "            local_times = []\n",
    "            global_times = []\n",
    "\n",
    "            for run in range(3):\n",
    "                spark = SparkSession.builder \\\n",
    "                    .master(\"spark://master:7077\") \\\n",
    "                    .appName(\"Group 14\") \\\n",
    "                    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-common:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "                    .config(\"spark.driver.host\", \"10.67.22.87\") \\\n",
    "                    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "                    .config(\"spark.cores.max\", max_cores) \\\n",
    "                    .config(\"spark.executor.instances\", executors) \\\n",
    "                    .config(\"spark.default.parallelism\", partitions) \\\n",
    "                    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "                    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.access.key\", \"4e0262c6d36a4fb4a627bbdfd4e28e8a\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.secret.key\", \"e009cac9320e4b5cbd99e21c28ef03cd\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.endpoint\", \"https://cloud-areapd.pd.infn.it:5210\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.metadatastore.impl\", \"org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "                    .config(\"com.amazonaws.sdk.disableCertChecking\", \"true\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "                sc = spark.sparkContext\n",
    "\n",
    "                # data loading\n",
    "                random.seed(42)\n",
    "                all_files = sc.binaryFiles(\"s3a://mapd-minidt-batch/data_*.dat\").keys().collect()\n",
    "                subset_size = int(0.3 * len(all_files))\n",
    "                random.shuffle(all_files)\n",
    "                subset_files = all_files[:subset_size]\n",
    "                file_path_str = \",\".join(subset_files)\n",
    "                \n",
    "\n",
    "###################################### u n p a c k #######################################################\n",
    "                \n",
    "                t0 = time.time()\n",
    "                rdd = sc.binaryFiles(file_path_str).flatMap(lambda f: unpack(f[1]))\n",
    "                rdd.count()\n",
    "                t_unpack = round(time.time() - t0, 3)\n",
    "\n",
    "                df = rdd.toDF(['FPGA','CHAN','ORBIT','BX','TDC'])\n",
    "\n",
    "                df = df.withColumn('CHAMBER', when((df.FPGA==0) & (df.CHAN < 63), 0)\\\n",
    "                                   .when((df.FPGA==0) & (df.CHAN > 63) & (df.CHAN < 128), 1)\\\n",
    "                                   .when((df.FPGA==1) & (df.CHAN < 63), 2)\\\n",
    "                                   .when((df.FPGA==1) & (df.CHAN > 63) & (df.CHAN < 128), 3)\\\n",
    "                                   .when((df.FPGA==1) & (df.CHAN ==128), 4)).na.drop()\n",
    "                \n",
    "                shift_chamber = {\n",
    "                    0: {'z': 219.8}, \n",
    "                    1: {'z': 977.3},\n",
    "                    2: {'z': 1035.6},\n",
    "                    3: {'z': 1819.8}\n",
    "                }\n",
    "                \n",
    "                df = df.withColumn('shift_z', when(col('CHAMBER') == 0, shift_chamber[0]['z'])\n",
    "                                   .when(col('CHAMBER') == 1, shift_chamber[1]['z'])\n",
    "                                   .when(col('CHAMBER') == 2, shift_chamber[2]['z'])\n",
    "                                   .when(col('CHAMBER') == 3, shift_chamber[3]['z'])\n",
    "                                   .otherwise(None))\n",
    "                \n",
    "                timecorrections = {0: 95.0 - 1.1, 1: 95.0 + 6.4, 2: 95.0 + 0.5, 3: 95.0 - 2.6}\n",
    "                            \n",
    "                df = df.withColumn('ABSOLUTETIME',\n",
    "                    25 * (col('ORBIT') * 3564 + col('BX') + col('TDC') / 30)\n",
    "                ).withColumn('T_HIT_ns',\n",
    "                    col('ABSOLUTETIME') + when(col('CHAMBER') == 0, timecorrections[0])\n",
    "                                           .when(col('CHAMBER') == 1, timecorrections[1])\n",
    "                                           .when(col('CHAMBER') == 2, timecorrections[2])\n",
    "                                           .when(col('CHAMBER') == 3, timecorrections[3])\n",
    "                                           .otherwise(0)\n",
    "                )\n",
    "\n",
    "                #scintillator hits, determines the start time of muon through detector\n",
    "                scintillator_hits = df.filter((col(\"CHAMBER\") == 4)) \\\n",
    "                                      .select(\"ORBIT\", \"T_HIT_ns\") \\\n",
    "                                      .withColumnRenamed(\"T_HIT_ns\", \"t0\")\n",
    "                \n",
    "                scintillator_counts = scintillator_hits.groupBy(\"ORBIT\").count() \\\n",
    "                                                       .withColumnRenamed(\"count\", \"hit_count\") \n",
    "\n",
    "\n",
    "                #dfnew: joining scintillator hits\n",
    "                dfnew = df.join(scintillator_hits, on=\"ORBIT\", how=\"inner\")\n",
    "\n",
    "                # Compute hit counts\n",
    "                hit_counts = dfnew.groupBy(\"ORBIT\", \"CHAMBER\").count()\n",
    "                total_hits_per_orbit = dfnew.groupBy(\"ORBIT\").count().withColumnRenamed(\"count\", \"total_hits\")\n",
    "                \n",
    "                # Filter ORBITs with hits in CHAMBERs 0, 2, and 3\n",
    "                orbits_with_ch0 = hit_counts.filter(F.col(\"CHAMBER\") == 0).select(\"ORBIT\").distinct()\n",
    "                orbits_with_ch2 = hit_counts.filter(F.col(\"CHAMBER\") == 2).select(\"ORBIT\").distinct()\n",
    "                orbits_with_ch3 = hit_counts.filter(F.col(\"CHAMBER\") == 3).select(\"ORBIT\").distinct()\n",
    "                \n",
    "                valid_orbits = orbits_with_ch0.intersect(orbits_with_ch2).intersect(orbits_with_ch3)\n",
    "                \n",
    "                # Filter ORBITs with < 15 hits\n",
    "                valid_orbits_with_hits = valid_orbits.join(total_hits_per_orbit, \"ORBIT\") \\\n",
    "                                                     .filter(F.col(\"total_hits\") < 15) \\\n",
    "                                                     .select(\"ORBIT\")\n",
    "                \n",
    "                \n",
    "                #filter keeping only oarbit\n",
    "                df_filtered = dfnew.join(valid_orbits_with_hits.select(\"ORBIT\"), on=\"ORBIT\", how=\"inner\")\n",
    "\n",
    "                # Calculate x positions and handle ambiguity\n",
    "                width = 42  # mm\n",
    "                height = 13  # mm\n",
    "                v_drift = 53.8 # Î¼m\n",
    "\n",
    "                # X_HIT\n",
    "                dfnew = df_filtered.withColumn(\n",
    "                        \"X_HIT\",\n",
    "                        (F.col(\"T_HIT_ns\") - F.col(\"t0\")) * (F.lit(v_drift) / 1000) # Convert um to mm, remove negative values\n",
    "                    )\n",
    "                \n",
    "                dfnew = dfnew.filter(F.col(\"X_HIT\").isNotNull()) #removing rows where X_HIT is null\n",
    "                \n",
    "                # x-centre of each channel\n",
    "                dfnew = dfnew.withColumn(\n",
    "                    'X_CENTRE',\n",
    "                    when(\n",
    "                        ((col('CHAMBER').isin(0, 2)) & (col('CHAN') % 4).isin(0, 1)),\n",
    "                        42 * F.floor(col('CHAN') / 4) + 21\n",
    "                    )\n",
    "                    .when(\n",
    "                        ((col('CHAMBER').isin(0, 2)) & (col('CHAN') % 4).isin(2, 3)),\n",
    "                        42 * F.floor(col('CHAN') / 4) + 42\n",
    "                    )\n",
    "                    .when(\n",
    "                        ((col('CHAMBER').isin(1, 3)) & (col('CHAN') % 4).isin(0, 1)),\n",
    "                        42 * F.floor((col('CHAN') - 64) / 4) + 21\n",
    "                    )\n",
    "                    .when(\n",
    "                        ((col('CHAMBER').isin(1, 3)) & (col('CHAN') % 4).isin(2, 3)),\n",
    "                        42 * F.floor((col('CHAN') - 64) / 4) + 42\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # X_LEFT and X_RIGHT based on X_CENTRE\n",
    "                dfnew = dfnew.withColumn('X_LEFT', col('X_CENTRE') - col('X_HIT'))\n",
    "                dfnew = dfnew.withColumn('X_RIGHT', col('X_CENTRE') + col('X_HIT'))\n",
    "                \n",
    "                # Adding Z coordinates for each channel in the local track\n",
    "                dfnew = dfnew.withColumn(\n",
    "                    'Z_LOCAL',\n",
    "                    when((col('CHAN') % 4 == 0), 19.5)\n",
    "                    .when((col('CHAN') % 4 == 1), -6.5)\n",
    "                    .when((col('CHAN') % 4 == 2), 6.5)\n",
    "                    .when((col('CHAN') % 4 == 3), -19.5)\n",
    "                )\n",
    "                \n",
    "                dfnew = dfnew.withColumn(\n",
    "                    'Z_GLOBAL',\n",
    "                    col('Z_LOCAL') + col('shift_z')\n",
    "                )\n",
    "                \n",
    "\n",
    "                df_ch = (\n",
    "                    dfnew\n",
    "                    .select(\"ORBIT\", \"CHAMBER\", \"X_LEFT\", \"X_RIGHT\", \"Z_GLOBAL\")\n",
    "                    .filter(\"X_LEFT IS NOT NULL AND X_RIGHT IS NOT NULL AND Z_GLOBAL IS NOT NULL\")\n",
    "                )\n",
    "                \n",
    "                # Convert to RDD with ((ORBIT, CHAMBER), [X_LEFT, X_RIGHT, Z_LOCAL])\n",
    "                first_rdd = df_ch.rdd.map(lambda row: ((row[\"ORBIT\"], row[\"CHAMBER\"]),\n",
    "                                                 [row[\"X_LEFT\"], row[\"X_RIGHT\"], row[\"Z_GLOBAL\"]]))\n",
    "                \n",
    "                # Combine by key to group the values into parallel arrays\n",
    "                grouped_rdd = first_rdd.combineByKey(\n",
    "                    lambda x: [[x[0]], [x[1]], [x[2]]],  # Create initial lists\n",
    "                    lambda acc, x: [acc[0] + [x[0]], acc[1] + [x[1]], acc[2] + [x[2]]],  # Merge in same partition\n",
    "                    lambda acc1, acc2: [acc1[0] + acc2[0], acc1[1] + acc2[1], acc1[2] + acc2[2]]  # Merge across partitions\n",
    "                )\n",
    "                \n",
    "\n",
    "###################################### l o c a l #######################################################\n",
    "\n",
    "                t0_local = time.time()\n",
    "\n",
    "                cleaned_rdd = grouped_rdd.mapValues(lambda vals: (\n",
    "                    lambda x_left, x_right, z_global: (\n",
    "                        # compute mask using X_CENTRE = (X_LEFT + X_RIGHT)/2 for better geometry filtering\n",
    "                        lambda mask: [\n",
    "                            list(np.array(x_left)[mask]),\n",
    "                            list(np.array(x_right)[mask]),\n",
    "                            list(np.array(z_global)[mask])\n",
    "                        ]\n",
    "                    )(remove_outliers_b((np.array(x_left) + np.array(x_right)) / 2))\n",
    "                )(vals[0], vals[1], vals[2]))\n",
    "                \n",
    "                rdd = cleaned_rdd.map(lambda kv: (\n",
    "                    (kv[0][1], kv[0][0]),  # key = (CHAMBER, ORBIT)\n",
    "                    (\n",
    "                        [kv[1][0], kv[1][1], kv[1][2]],  # cleaned lists\n",
    "                        best_fit_rdd(zip(kv[1][0], kv[1][1], kv[1][2]))  # fit function\n",
    "                    )\n",
    "                ))\n",
    "                \n",
    "                # keep only orbits with 3+ remaining hits\n",
    "                rdd = rdd.filter(lambda x: len(x[1][0][0]) > 2)\n",
    "\n",
    "                rdd.count() \n",
    "\n",
    "                t_local = round(time.time() - t0_local, 3)\n",
    "\n",
    "                \n",
    "            \n",
    "###################################### g l o b a l #######################################################\n",
    "                \n",
    "                t0_global = time.time()\n",
    "\n",
    "                global_fit_rdd = rdd.filter(lambda x: x[0][0] in [0, 2, 3])\n",
    "                global_fit_rdd = global_fit_rdd.map(lambda x: (\n",
    "                    x[0][1],  # ORBIT is the key\n",
    "                    list(zip(x[1][1][3], x[1][0][2]))  # (X_hit, Z) tuples\n",
    "                ))\n",
    "                global_fit_rdd = global_fit_rdd.reduceByKey(lambda a, b: a + b)\n",
    "                global_fit_rdd = global_fit_rdd.filter(lambda x: len(x[1]) >= 3)\n",
    "                global_rdd = global_fit_rdd.mapValues(linear_fit)\n",
    "                \n",
    "                global_rdd.count()  \n",
    "                \n",
    "                t_global = round(time.time() - t0_global, 3)\n",
    "\n",
    "                \n",
    "###################################### per-run times #######################################################\n",
    "                \n",
    "                unpack_times.append(t_unpack)\n",
    "                local_times.append(t_local)\n",
    "                global_times.append(t_global)\n",
    "\n",
    "                # Cleanup + time between runs\n",
    "                os.system(\"rm -f /path/to/logs/*.log\")\n",
    "                os.system(\"rm -rf /tmp/spark-*\")\n",
    "                spark.stop()\n",
    "                time.sleep(10)\n",
    "\n",
    "            \n",
    "###################################### avergages of 3 runs #######################################################\n",
    "            \n",
    "            avg_unpack = sum(unpack_times) / len(unpack_times)\n",
    "            avg_local = sum(local_times) / len(local_times)\n",
    "            avg_global = sum(global_times) / len(global_times)\n",
    "\n",
    "            results.append({\n",
    "                \"executors\": executors,\n",
    "                \"cores_per_executor\": max_cores,\n",
    "                \"partitions\": partitions,\n",
    "                \"unpack_time_avg_s\": avg_unpack,\n",
    "                \"local_time_avg_s\": avg_local,\n",
    "                \"global_time_avg_s\": avg_global,\n",
    "                \"unpack_runs\": unpack_times,\n",
    "                \"local_runs\": local_times,\n",
    "                \"global_runs\": global_times\n",
    "            })\n",
    "\n",
    "\n",
    "            print(f\"Done | Executors: {executors}, Cores: {max_cores}, Partitions: {partitions} | \"\n",
    "                 f\"Avg Unpack: {avg_unpack:.3f}s | Avg Local: {avg_local:.3f}s | Avg Global: {avg_global:.3f}s\")\n",
    "\n",
    "\n",
    "\n",
    "###################################### e x p o r t #######################################################\n",
    "\n",
    "\n",
    "pd.DataFrame(results).to_csv(\"5.csv\", index=False)\n",
    "print(\"donenenenen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e18b737-ecc0-4df3-820a-af60a524bcc3",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
